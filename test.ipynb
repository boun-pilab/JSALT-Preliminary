{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_pose(points, \n",
    "              image_size=(135, 135), \n",
    "              padding=1, \n",
    "              point_radius=2, \n",
    "              line_thickness=2, \n",
    "              color=(0, 0, 255), \n",
    "              file_name='test'):\n",
    "    \"\"\"\n",
    "    Draws 2D pose points on a white background image.\n",
    "\n",
    "    :param points: List of tuples (x, y) representing the keypoints.\n",
    "    :param image_size: Tuple (width, height) of the output image.\n",
    "    :param point_radius: Radius of the circle to draw for each point.\n",
    "    :param line_thickness: Thickness of the line to connect keypoints.\n",
    "    :param color: Color of the points and lines (B, G, R).\n",
    "    :return: Image with pose drawn.\n",
    "    \"\"\"\n",
    "    # Create a white background image\n",
    "    image = np.full((image_size[1], image_size[0], 3), 255, dtype=np.uint8)\n",
    "\n",
    "    # Convert normalized points (-1 to 1) to pixel coordinates\n",
    "    scaled_points = [(int((x + 1) * 0.3 * (image_size[0])), int((y + 1) * 0.3 * (image_size[1]))) for x, y, z in points]\n",
    "\n",
    "    # Add custum connections\n",
    "    # Draw lines between points (optional, depends on the structure of your points)\n",
    "    # for i in range(len(scaled_points) - 1):\n",
    "    #     rand_color = (random.randint(0,255), random.randint(0,255),random.randint(0,255))\n",
    "    #     cv2.line(image, scaled_points[i], scaled_points[i+1], rand_color, line_thickness)\n",
    "\n",
    "    # Draw points\n",
    "    for point in scaled_points:\n",
    "        rand_color = (random.randint(0,255), random.randint(0,255),random.randint(0,255))\n",
    "        cv2.circle(image, point, point_radius, rand_color, -1)  # -1 fills the circle\n",
    "\n",
    "    plt.imsave(f'{file_name}.png', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example pose points (you can update this with real data)\n",
    "# pose_points = [(100, 100), (150, 200), (200, 300), (250, 100), (300, 200)]\n",
    "\n",
    "# # Generate the image\n",
    "# draw_pose(pose_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "POSE_DATA_PATH = 'toy-dataset/pose/'\n",
    "FILES = glob.glob(os.path.join(POSE_DATA_PATH, '*.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def open_poses(pose_file = FILES[1]):\n",
    "\n",
    "    with open(pose_file,'rb') as f:\n",
    "        pose_df = np.load(f)\n",
    "        pose_df = pd.DataFrame(pose_df)\n",
    "        pose_df = pose_df.replace(np.nan, 0)\n",
    "\n",
    "    return pose_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = open_poses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = df.iloc[35].to_numpy()\n",
    "arr = arr.reshape(int(arr.shape[0] / 3), 3)\n",
    "\n",
    "draw_pose(\n",
    "    points=arr,\n",
    "    image_size=(2120, 2120), \n",
    "    padding=1, \n",
    "    point_radius=5, \n",
    "    line_thickness=5, \n",
    "    color=(0, 0, 255), \n",
    "    file_name='test'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Wav2Vec2 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (14.0.2)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (4.65.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, dill, multiprocess, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "Successfully installed datasets-2.19.1 dill-0.3.8 multiprocess-0.70.16 pyarrow-hotfix-0.6 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install 'urllib3<2' soundfile librosa torch transformers torchaudio\n",
    "!pip3 install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2ForPreTraining\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices\n",
    "from datasets import load_dataset\n",
    "\n",
    "def sample_infer():\n",
    "\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "    model = Wav2Vec2ForPreTraining.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "    ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "    input_values = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values  # Batch size 1\n",
    "\n",
    "    # compute masked indices\n",
    "    batch_size, raw_sequence_length = input_values.shape\n",
    "    sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length).item()\n",
    "    mask_time_indices = _compute_mask_indices(\n",
    "        shape=(batch_size, sequence_length), mask_prob=0.2, mask_length=2\n",
    "    )\n",
    "    sampled_negative_indices = _sample_negative_indices(\n",
    "        features_shape=(batch_size, sequence_length),\n",
    "        num_negatives=model.config.num_negatives,\n",
    "        mask_time_indices=mask_time_indices,\n",
    "    )\n",
    "    mask_time_indices = torch.tensor(data=mask_time_indices, device=input_values.device, dtype=torch.long)\n",
    "    sampled_negative_indices = torch.tensor(\n",
    "        data=sampled_negative_indices, device=input_values.device, dtype=torch.long\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values, mask_time_indices=mask_time_indices)\n",
    "\n",
    "    # compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)\n",
    "    cosine_sim = torch.cosine_similarity(outputs.projected_states, outputs.projected_quantized_states, dim=-1)\n",
    "\n",
    "    # show that cosine similarity is much higher than random\n",
    "    cosine_sim[mask_time_indices.to(torch.bool)].mean() > 0.5\n",
    "\n",
    "    # for contrastive loss training model should be put into train mode\n",
    "    model = model.train()\n",
    "\n",
    "    output = model(\n",
    "        input_values, mask_time_indices=mask_time_indices, sampled_negative_indices=sampled_negative_indices\n",
    "    )\n",
    "\n",
    "\n",
    "    return feature_extractor, model, ds, input_values, mask_time_indices, sampled_negative_indices, outputs, cosine_sim, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForPreTraining: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForPreTraining from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForPreTraining from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForPreTraining were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.11/site-packages/datasets/load.py:1486: FutureWarning: The repository for hf-internal-testing/librispeech_asr_dummy contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hf-internal-testing/librispeech_asr_dummy\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "feature_extractor, model, ds, input_values, mask_time_indices, sampled_negative_indices, outputs, cosine_sim, output = sample_infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.sign2vec import Sign2VecFeatureEncoder\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2FeatureProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video conv 1d layer to process the video input\n",
    "# (B x C x T x H x W) -> (B x C x T x H x W)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "    batch_size = 1\n",
    "    channels = 3\n",
    "    time = 512\n",
    "    height = 75\n",
    "    width = 75\n",
    "\n",
    "    input_video = torch.randn(\n",
    "        batch_size, channels, time, height, width\n",
    "    )\n",
    "\n",
    "    print('INPUT_SHAPE', input_video.shape)\n",
    "\n",
    "    output = nn.Conv3d(\n",
    "        in_channels=3,\n",
    "        out_channels=10,\n",
    "        kernel_size=(10, 10, 10),\n",
    "        stride=(1, 1, 1),\n",
    "        padding=(0, 1, 1),\n",
    "        dilation=(1, 1, 1),\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "        padding_mode='zeros'\n",
    "    )(input_video)\n",
    "\n",
    "    print('LAYER_O OUTPUT_SHAPE',output.shape)\n",
    "\n",
    "    output = nn.Conv3d(\n",
    "        in_channels=10,\n",
    "        out_channels=20,\n",
    "        kernel_size=(10, output.shape[3], output.shape[4]),\n",
    "        stride=(1, 1, 1),\n",
    "        padding=(0, 1, 1),\n",
    "        dilation=(1, 1, 1),\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "        padding_mode='zeros'\n",
    "    )(output)\n",
    "\n",
    "    print('LAYER_1 OUTPUT_SHAPE',output.shape)\n",
    "\n",
    "    output = nn.Conv3d(\n",
    "        in_channels=20,\n",
    "        out_channels=50,\n",
    "        kernel_size=(10, output.shape[3], output.shape[4]),\n",
    "        stride=(1, 1, 1),\n",
    "        padding=(0, 1, 1),\n",
    "        dilation=(1, 1, 1),\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "        padding_mode='zeros'\n",
    "    )(output)\n",
    "\n",
    "    print('LAYER_1 OUTPUT_SHAPE',output.shape)\n",
    "\n",
    "    # output = output.reshape(output.shape[0] * output.shape[1], output.shape[2], output.shape[3], output.shape[4])\n",
    "    output = output.transpose(1,2)\n",
    "    # merge channel and (height, width) dimensions with einsum\n",
    "    output = output.reshape(output.shape[0], output.shape[1], -1)\n",
    "\n",
    "    print('LAYER_1 OUTPUT_SHAPE',output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_3d_layer_dict = [\n",
    "    { 'in_channels': 3,  'out_channels': 10, 'kernel_size': ( 5,  5,  5 ), 'stride': (1, 1, 1), 'padding': (0, 1, 1) }, \n",
    "    { 'in_channels': 10, 'out_channels': 20, 'kernel_size': ( 2,  5,  5 ), 'stride': (1, 2, 2), 'padding': (0, 1, 1) }, \n",
    "    { 'in_channels': 20, 'out_channels': 20, 'kernel_size': ( 2,  2,  2 ), 'stride': (1, 2, 2), 'padding': (0, 1, 1) }, \n",
    "    { 'in_channels': 20, 'out_channels': 20, 'kernel_size': ( 2,  2,  2 ), 'stride': (1, 2, 2), 'padding': (0, 1, 1) }, \n",
    "    { 'in_channels': 20, 'out_channels': 20, 'kernel_size': ( 1,  2,  2 ), 'stride': (1, 1, 1), 'padding': (0, 1, 1) }, \n",
    "    { 'in_channels': 20, 'out_channels': 30, 'kernel_size': ( 1,  2,  1 ), 'stride': (1, 1, 1), 'padding': (0, 1, 1) }, \n",
    "    { 'in_channels': 30, 'out_channels': 5,  'kernel_size': ( 1,  2,  2 ), 'stride': (1, 1, 1), 'padding': (0, 1, 1) }, \n",
    "    { 'in_channels': 30, 'out_channels': 5,  'kernel_size': ( 2,  2,  2 ), 'stride': (1, 1, 1), 'padding': (0, 1, 1) }, \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_channels = []\n",
    "conv_kernel = []\n",
    "conv_stride = []\n",
    "\n",
    "for layer in conv_3d_layer_dict:\n",
    "    conv_channels.append(layer['out_channels'])\n",
    "    conv_kernel.append(layer['kernel_size'])\n",
    "    conv_stride.append(layer['stride'])\n",
    "\n",
    "\n",
    "config.conv_3d_channels = conv_channels\n",
    "config.conv_3d_kernel = conv_kernel\n",
    "config.conv_3d_stride = conv_stride\n",
    "config.num_3d_feat_extract_layers = len(conv_3d_layer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.activations import ACT2FN\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2GroupNormConvLayer,\n",
    "    Wav2Vec2LayerNormConvLayer,\n",
    "    Wav2Vec2NoLayerNormConvLayer,\n",
    ")\n",
    "\n",
    "from lib.utils.sign2vec import (\n",
    "    Sign2VecGroupNormConvLayer,\n",
    "    Sign2VecLayerNormConvLayer,\n",
    "    Sign2VecNoLayerNormConvLayer,\n",
    ")\n",
    "\n",
    "class Sign2VecFeatureEncoder(nn.Module):\n",
    "    \"\"\"Construct the features from raw audio waveform\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # 3D Convolutional Layers - to spatio-temporally downsample the input\n",
    "        if config.feat_extract_norm == \"group\":\n",
    "            conv_layers = [Sign2VecGroupNormConvLayer(config, layer_id=0)] + [\n",
    "                Sign2VecNoLayerNormConvLayer(config, layer_id=i + 1) for i in range(config.num_3d_feat_extract_layers - 1)\n",
    "            ]\n",
    "        elif config.feat_extract_norm == \"layer\":\n",
    "            conv_layers = [\n",
    "                Sign2VecLayerNormConvLayer(config, layer_id=i) for i in range(config.num_3d_feat_extract_layers)\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"`config.feat_extract_norm` is {config.feat_extract_norm}, but has to be one of ['group', 'layer']\"\n",
    "            )\n",
    "        \n",
    "        self.conv_3d_layers = nn.ModuleList(conv_layers)\n",
    "\n",
    "\n",
    "        if config.feat_extract_norm == \"group\":\n",
    "            conv_layers =  [\n",
    "                Wav2Vec2NoLayerNormConvLayer(config, layer_id=i + 1) for i in range(config.num_feat_extract_layers - 1)\n",
    "            ]\n",
    "        elif config.feat_extract_norm == \"layer\":\n",
    "            conv_layers = [\n",
    "                Wav2Vec2LayerNormConvLayer(config, layer_id=i) for i in range(config.num_feat_extract_layers)\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"`config.feat_extract_norm` is {config.feat_extract_norm}, but has to be one of ['group', 'layer']\"\n",
    "            )\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList(conv_layers)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        self._requires_grad = True\n",
    "\n",
    "    def _freeze_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._requires_grad = False\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states: (batch_size, channels, time_steps, height, width)\n",
    "        # make sure hidden_states require grad for gradient_checkpointing\n",
    "        if self._requires_grad and self.training:\n",
    "            hidden_states.requires_grad = True\n",
    "\n",
    "        for ix, conv_layer in enumerate(self.conv_3d_layers):\n",
    "            if self._requires_grad and self.gradient_checkpointing and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                hidden_states = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(conv_layer),\n",
    "                    hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                hidden_states = conv_layer(hidden_states)\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1,2)\n",
    "        # merge (channel) and (height, width) dimensions\n",
    "        hidden_states = hidden_states.reshape(hidden_states.shape[0], hidden_states.shape[1], -1)\n",
    "        hidden_states = hidden_states.transpose(1,2)\n",
    "\n",
    "        print('HIDDEN_STATES', hidden_states.shape)\n",
    "\n",
    "        for ix, conv_layer in enumerate(self.conv_layers):\n",
    "            if self._requires_grad and self.gradient_checkpointing and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                hidden_states = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(conv_layer),\n",
    "                    hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                hidden_states = conv_layer(hidden_states)\n",
    "\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.conv_dim[0] = 660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Sign2VecFeatureEncoder(config)\n",
    "feature_projection = Wav2Vec2FeatureProjection(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d LAYER: 0\n",
      "LAYER_INPUT: torch.Size([1, 3, 256, 128, 128])\n",
      "CONV_LAYER: Sign2VecGroupNormConvLayer(\n",
      "  (conv): Conv3d(3, 10, kernel_size=(5, 5, 5), stride=(1, 1, 1), bias=False)\n",
      "  (activation): GELUActivation()\n",
      "  (layer_norm): GroupNorm(10, 10, eps=1e-05, affine=True)\n",
      ")\n",
      "-------------------\n",
      "3d LAYER: 1\n",
      "LAYER_INPUT: torch.Size([1, 10, 252, 124, 124])\n",
      "CONV_LAYER: Sign2VecNoLayerNormConvLayer(\n",
      "  (conv): Conv3d(10, 20, kernel_size=(2, 5, 5), stride=(1, 2, 2), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n",
      "3d LAYER: 2\n",
      "LAYER_INPUT: torch.Size([1, 20, 251, 60, 60])\n",
      "CONV_LAYER: Sign2VecNoLayerNormConvLayer(\n",
      "  (conv): Conv3d(20, 20, kernel_size=(2, 2, 2), stride=(1, 2, 2), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n",
      "3d LAYER: 3\n",
      "LAYER_INPUT: torch.Size([1, 20, 250, 30, 30])\n",
      "CONV_LAYER: Sign2VecNoLayerNormConvLayer(\n",
      "  (conv): Conv3d(20, 20, kernel_size=(2, 2, 2), stride=(1, 2, 2), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n",
      "3d LAYER: 4\n",
      "LAYER_INPUT: torch.Size([1, 20, 249, 15, 15])\n",
      "CONV_LAYER: Sign2VecNoLayerNormConvLayer(\n",
      "  (conv): Conv3d(20, 20, kernel_size=(1, 2, 2), stride=(1, 1, 1), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n",
      "3d LAYER: 5\n",
      "LAYER_INPUT: torch.Size([1, 20, 249, 14, 14])\n",
      "CONV_LAYER: Sign2VecNoLayerNormConvLayer(\n",
      "  (conv): Conv3d(20, 30, kernel_size=(1, 2, 1), stride=(1, 1, 1), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n",
      "3d LAYER: 6\n",
      "LAYER_INPUT: torch.Size([1, 30, 249, 13, 14])\n",
      "CONV_LAYER: Sign2VecNoLayerNormConvLayer(\n",
      "  (conv): Conv3d(30, 5, kernel_size=(1, 2, 2), stride=(1, 1, 1), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n",
      "3d LAYER: 7\n",
      "LAYER_INPUT: torch.Size([1, 5, 249, 12, 13])\n",
      "CONV_LAYER: Sign2VecNoLayerNormConvLayer(\n",
      "  (conv): Conv3d(5, 5, kernel_size=(2, 2, 2), stride=(1, 1, 1), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n",
      "1d LAYER: 0\n",
      "CONV_LAYER: Wav2Vec2NoLayerNormConvLayer(\n",
      "  (conv): Conv1d(660, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n",
      "1d LAYER: 1\n",
      "CONV_LAYER: Wav2Vec2NoLayerNormConvLayer(\n",
      "  (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n",
      "1d LAYER: 2\n",
      "CONV_LAYER: Wav2Vec2NoLayerNormConvLayer(\n",
      "  (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n",
      "1d LAYER: 3\n",
      "CONV_LAYER: Wav2Vec2NoLayerNormConvLayer(\n",
      "  (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n",
      "1d LAYER: 4\n",
      "CONV_LAYER: Wav2Vec2NoLayerNormConvLayer(\n",
      "  (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n",
      "1d LAYER: 5\n",
      "CONV_LAYER: Wav2Vec2NoLayerNormConvLayer(\n",
      "  (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "channels = 3\n",
    "time = 256\n",
    "height = 128\n",
    "width = 128\n",
    "\n",
    "input_video = torch.randn(\n",
    "    batch_size, channels, time, height, width\n",
    ")\n",
    "\n",
    "extract_features = feature_extractor(input_video)\n",
    "extract_features.shape  # (batch_size, num_3d_feat_extract_layers, T, H, W)\n",
    "# extract_features = extract_features.transpose(1, 2)\n",
    "\n",
    "# extract_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 292])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2FeatureEncoder\n",
    "\n",
    "feature_extractor_audio = Wav2Vec2FeatureEncoder(config)\n",
    "\n",
    "with torch.no_grad():\n",
    "    extract_features_audio = feature_extractor_audio(input_values)\n",
    "\n",
    "extract_features_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extract_features = feature_extractor(input_video)\n",
    "extract_features.shape\n",
    "# extract_features = extract_features.transpose(1, 2)\n",
    "\n",
    "# extract_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 93680])\n",
      "torch.Size([1, 292])\n",
      "torch.Size([1, 292, 100])\n"
     ]
    }
   ],
   "source": [
    "print(input_values.shape)\n",
    "print(mask_time_indices.shape)\n",
    "print(sampled_negative_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 30.22791290283203\n",
      "---\n",
      "projected_states torch.Size([1, 292, 256])\n",
      "---\n",
      "projected_quantized_states torch.Size([1, 292, 256])\n",
      "---\n",
      "codevector_perplexity 100.37718200683594\n",
      "---\n",
      "hidden_states no shape\n",
      "---\n",
      "attentions no shape\n",
      "---\n",
      "contrastive_loss 25.59052848815918\n",
      "---\n",
      "diversity_loss 46.37383270263672\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for val in output.__dict__.keys():\n",
    "    try:\n",
    "        print(val, output[val].size() if len(output[val].size()) > 0 else output[val].item())\n",
    "    except:\n",
    "        print(val, 'no shape')\n",
    "    print('---')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
