{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_pose(points, \n",
    "              image_size=(135, 135), \n",
    "              padding=1, \n",
    "              point_radius=2, \n",
    "              line_thickness=2, \n",
    "              color=(0, 0, 255), \n",
    "              file_name='test'):\n",
    "    \"\"\"\n",
    "    Draws 2D pose points on a white background image.\n",
    "\n",
    "    :param points: List of tuples (x, y) representing the keypoints.\n",
    "    :param image_size: Tuple (width, height) of the output image.\n",
    "    :param point_radius: Radius of the circle to draw for each point.\n",
    "    :param line_thickness: Thickness of the line to connect keypoints.\n",
    "    :param color: Color of the points and lines (B, G, R).\n",
    "    :return: Image with pose drawn.\n",
    "    \"\"\"\n",
    "    # Create a white background image\n",
    "    image = np.full((image_size[1], image_size[0], 3), 255, dtype=np.uint8)\n",
    "\n",
    "    # Convert normalized points (-1 to 1) to pixel coordinates\n",
    "    scaled_points = [(int((x + 1) * 0.3 * (image_size[0])), int((y + 1) * 0.3 * (image_size[1]))) for x, y, z in points]\n",
    "\n",
    "    # Add custum connections\n",
    "    # Draw lines between points (optional, depends on the structure of your points)\n",
    "    # for i in range(len(scaled_points) - 1):\n",
    "    #     rand_color = (random.randint(0,255), random.randint(0,255),random.randint(0,255))\n",
    "    #     cv2.line(image, scaled_points[i], scaled_points[i+1], rand_color, line_thickness)\n",
    "\n",
    "    # Draw points\n",
    "    for point in scaled_points:\n",
    "        rand_color = (random.randint(0,255), random.randint(0,255),random.randint(0,255))\n",
    "        cv2.circle(image, point, point_radius, rand_color, -1)  # -1 fills the circle\n",
    "\n",
    "    plt.imsave(f'{file_name}.png', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example pose points (you can update this with real data)\n",
    "# pose_points = [(100, 100), (150, 200), (200, 300), (250, 100), (300, 200)]\n",
    "\n",
    "# # Generate the image\n",
    "# draw_pose(pose_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "POSE_DATA_PATH = 'toy-dataset/pose/'\n",
    "FILES = glob.glob(os.path.join(POSE_DATA_PATH, '*.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def open_poses(pose_file = FILES[1]):\n",
    "\n",
    "    with open(pose_file,'rb') as f:\n",
    "        pose_df = np.load(f)\n",
    "        pose_df = pd.DataFrame(pose_df)\n",
    "        pose_df = pose_df.replace(np.nan, 0)\n",
    "\n",
    "    return pose_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = open_poses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = df.iloc[35].to_numpy()\n",
    "arr = arr.reshape(int(arr.shape[0] / 3), 3)\n",
    "\n",
    "draw_pose(\n",
    "    points=arr,\n",
    "    image_size=(2120, 2120), \n",
    "    padding=1, \n",
    "    point_radius=5, \n",
    "    line_thickness=5, \n",
    "    color=(0, 0, 255), \n",
    "    file_name='test'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Wav2Vec2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install 'urllib3<2' soundfile librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kara-nlp/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForPreTraining: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForPreTraining from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForPreTraining from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForPreTraining were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Found cached dataset librispeech_asr_dummy (/home/kara-nlp/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "/home/kara-nlp/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2ForPreTraining\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices\n",
    "from datasets import load_dataset\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2ForPreTraining.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "input_values = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values  # Batch size 1\n",
    "\n",
    "# compute masked indices\n",
    "batch_size, raw_sequence_length = input_values.shape\n",
    "sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length).item()\n",
    "mask_time_indices = _compute_mask_indices(\n",
    "    shape=(batch_size, sequence_length), mask_prob=0.2, mask_length=2\n",
    ")\n",
    "sampled_negative_indices = _sample_negative_indices(\n",
    "    features_shape=(batch_size, sequence_length),\n",
    "    num_negatives=model.config.num_negatives,\n",
    "    mask_time_indices=mask_time_indices,\n",
    ")\n",
    "mask_time_indices = torch.tensor(data=mask_time_indices, device=input_values.device, dtype=torch.long)\n",
    "sampled_negative_indices = torch.tensor(\n",
    "    data=sampled_negative_indices, device=input_values.device, dtype=torch.long\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_values, mask_time_indices=mask_time_indices)\n",
    "\n",
    "# compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)\n",
    "cosine_sim = torch.cosine_similarity(outputs.projected_states, outputs.projected_quantized_states, dim=-1)\n",
    "\n",
    "# show that cosine similarity is much higher than random\n",
    "cosine_sim[mask_time_indices.to(torch.bool)].mean() > 0.5\n",
    "\n",
    "# for contrastive loss training model should be put into train mode\n",
    "model = model.train()\n",
    "\n",
    "output = model(\n",
    "    input_values, mask_time_indices=mask_time_indices, sampled_negative_indices=sampled_negative_indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lib.utils.sign2vec import Sign2VecFeatureEncoder\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2FeatureProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video conv 1d layer to process the video input\n",
    "# (B x C x T x H x W) -> (B x C x T x H x W)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "    batch_size = 1\n",
    "    channels = 3\n",
    "    time = 512\n",
    "    height = 75\n",
    "    width = 75\n",
    "\n",
    "    input_video = torch.randn(\n",
    "        batch_size, channels, time, height, width\n",
    "    )\n",
    "\n",
    "    print('INPUT_SHAPE', input_video.shape)\n",
    "\n",
    "    output = nn.Conv3d(\n",
    "        in_channels=3,\n",
    "        out_channels=10,\n",
    "        kernel_size=(10, 10, 10),\n",
    "        stride=(1, 1, 1),\n",
    "        padding=(0, 1, 1),\n",
    "        dilation=(1, 1, 1),\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "        padding_mode='zeros'\n",
    "    )(input_video)\n",
    "\n",
    "    print('LAYER_O OUTPUT_SHAPE',output.shape)\n",
    "\n",
    "    output = nn.Conv3d(\n",
    "        in_channels=10,\n",
    "        out_channels=20,\n",
    "        kernel_size=(10, output.shape[3], output.shape[4]),\n",
    "        stride=(1, 1, 1),\n",
    "        padding=(0, 1, 1),\n",
    "        dilation=(1, 1, 1),\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "        padding_mode='zeros'\n",
    "    )(output)\n",
    "\n",
    "    print('LAYER_1 OUTPUT_SHAPE',output.shape)\n",
    "\n",
    "    output = nn.Conv3d(\n",
    "        in_channels=20,\n",
    "        out_channels=50,\n",
    "        kernel_size=(10, output.shape[3], output.shape[4]),\n",
    "        stride=(1, 1, 1),\n",
    "        padding=(0, 1, 1),\n",
    "        dilation=(1, 1, 1),\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "        padding_mode='zeros'\n",
    "    )(output)\n",
    "\n",
    "    print('LAYER_1 OUTPUT_SHAPE',output.shape)\n",
    "\n",
    "    # output = output.reshape(output.shape[0] * output.shape[1], output.shape[2], output.shape[3], output.shape[4])\n",
    "    output = output.transpose(1,2)\n",
    "    # merge channel and (height, width) dimensions with einsum\n",
    "    output = output.reshape(output.shape[0], output.shape[1], -1)\n",
    "\n",
    "    print('LAYER_1 OUTPUT_SHAPE',output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_3d_layer_dict = [\n",
    "    { 'in_channels': 3,  'out_channels': 10, 'kernel_size': ( 5, 5, 5 ), 'stride': (1, 1, 1), 'padding': (0, 1, 1) }, \n",
    "    { 'in_channels': 10, 'out_channels': 20, 'kernel_size': ( 2, 2, 2 ), 'stride': (1, 2, 2), 'padding': (0, 1, 1) }, \n",
    "    { 'in_channels': 20, 'out_channels': 20, 'kernel_size': ( 2, 2, 2 ), 'stride': (1, 2, 2), 'padding': (0, 1, 1) }, \n",
    "    { 'in_channels': 20, 'out_channels': 20, 'kernel_size': ( 2, 2, 2 ), 'stride': (1, 2, 2), 'padding': (0, 1, 1) }, \n",
    "    { 'in_channels': 20, 'out_channels': 20, 'kernel_size': ( 1, 1, 1 ), 'stride': (1, 1, 1), 'padding': (0, 1, 1) }, \n",
    "    { 'in_channels': 20, 'out_channels': 30, 'kernel_size': ( 1, 1, 1 ), 'stride': (1, 1, 1), 'padding': (0, 1, 1) }, \n",
    "    { 'in_channels': 30, 'out_channels': 5,  'kernel_size': ( 1, 1, 1 ), 'stride': (1, 1, 1), 'padding': (0, 1, 1) }, \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_channels = []\n",
    "conv_kernel = []\n",
    "conv_stride = []\n",
    "\n",
    "for layer in conv_3d_layer_dict:\n",
    "    conv_channels.append(layer['out_channels'])\n",
    "    conv_kernel.append(layer['kernel_size'])\n",
    "    conv_stride.append(layer['stride'])\n",
    "\n",
    "\n",
    "config.conv_3d_channels = conv_channels\n",
    "config.conv_3d_kernel = conv_kernel\n",
    "config.conv_3d_stride = conv_stride\n",
    "config.num_3d_feat_extract_layers = len(conv_3d_layer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.activations import ACT2FN\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2GroupNormConvLayer,\n",
    "    Wav2Vec2LayerNormConvLayer,\n",
    "    Wav2Vec2NoLayerNormConvLayer,\n",
    ")\n",
    "\n",
    "from lib.utils.sign2vec import (\n",
    "    Sign2VecGroupNormConvLayer,\n",
    "    Sign2VecLayerNormConvLayer,\n",
    "    Sign2VecNoLayerNormConvLayer,\n",
    ")\n",
    "\n",
    "class Sign2VecFeatureEncoder(nn.Module):\n",
    "    \"\"\"Construct the features from raw audio waveform\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # 3D Convolutional Layers - to spatio-temporally downsample the input\n",
    "        if config.feat_extract_norm == \"group\":\n",
    "            conv_layers = [Sign2VecGroupNormConvLayer(config, layer_id=0)] + [\n",
    "                Sign2VecNoLayerNormConvLayer(config, layer_id=i + 1) for i in range(config.num_3d_feat_extract_layers - 1)\n",
    "            ]\n",
    "        elif config.feat_extract_norm == \"layer\":\n",
    "            conv_layers = [\n",
    "                Sign2VecLayerNormConvLayer(config, layer_id=i) for i in range(config.num_3d_feat_extract_layers)\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"`config.feat_extract_norm` is {config.feat_extract_norm}, but has to be one of ['group', 'layer']\"\n",
    "            )\n",
    "        \n",
    "        self.conv_3d_layers = nn.ModuleList(conv_layers)\n",
    "\n",
    "\n",
    "        if config.feat_extract_norm == \"group\":\n",
    "            conv_layers =  [\n",
    "                Wav2Vec2NoLayerNormConvLayer(config, layer_id=i + 1) for i in range(config.num_feat_extract_layers - 1)\n",
    "            ]\n",
    "        elif config.feat_extract_norm == \"layer\":\n",
    "            conv_layers = [\n",
    "                Wav2Vec2LayerNormConvLayer(config, layer_id=i) for i in range(config.num_feat_extract_layers)\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"`config.feat_extract_norm` is {config.feat_extract_norm}, but has to be one of ['group', 'layer']\"\n",
    "            )\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList(conv_layers)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        self._requires_grad = True\n",
    "\n",
    "    def _freeze_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._requires_grad = False\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states: (batch_size, channels, time_steps, height, width)\n",
    "        # make sure hidden_states require grad for gradient_checkpointing\n",
    "        if self._requires_grad and self.training:\n",
    "            hidden_states.requires_grad = True\n",
    "\n",
    "        for ix, conv_layer in enumerate(self.conv_3d_layers):\n",
    "            \n",
    "            print(f'3d LAYER: {ix}')\n",
    "            print('LAYER_INPUT:', hidden_states.shape)  \n",
    "            print('CONV_LAYER:',conv_layer)\n",
    "            print('-------------------')\n",
    "            if self._requires_grad and self.gradient_checkpointing and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                hidden_states = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(conv_layer),\n",
    "                    hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                hidden_states = conv_layer(hidden_states)\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1,2)\n",
    "        # merge (channel) and (height, width) dimensions\n",
    "        hidden_states = hidden_states.reshape(hidden_states.shape[0], hidden_states.shape[1], -1)\n",
    "        hidden_states = hidden_states.transpose(1,2)\n",
    "\n",
    "        for ix, conv_layer in enumerate(self.conv_layers):\n",
    "\n",
    "            print(f'1d LAYER: {ix}')\n",
    "            print('LAYER_INPUT:', hidden_states.shape)  \n",
    "            print('CONV_LAYER:',conv_layer)\n",
    "            print('-------------------')\n",
    "\n",
    "            if self._requires_grad and self.gradient_checkpointing and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                hidden_states = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(conv_layer),\n",
    "                    hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                hidden_states = conv_layer(hidden_states)\n",
    "\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Sign2VecFeatureEncoder(config)\n",
    "feature_projection = Wav2Vec2FeatureProjection(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d LAYER: 0\n",
      "LAYER_INPUT: torch.Size([1, 3, 256, 128, 128])\n",
      "CONV_LAYER: Sign2VecGroupNormConvLayer(\n",
      "  (conv): Conv3d(3, 10, kernel_size=(5, 5, 5), stride=(1, 1, 1), bias=False)\n",
      "  (activation): GELUActivation()\n",
      "  (layer_norm): GroupNorm(10, 10, eps=1e-05, affine=True)\n",
      ")\n",
      "-------------------\n",
      "3d LAYER: 1\n",
      "LAYER_INPUT: torch.Size([1, 10, 252, 124, 124])\n",
      "CONV_LAYER: Sign2VecNoLayerNormConvLayer(\n",
      "  (conv): Conv3d(10, 20, kernel_size=(2, 2, 2), stride=(1, 2, 2), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n",
      "3d LAYER: 2\n",
      "LAYER_INPUT: torch.Size([1, 20, 251, 62, 62])\n",
      "CONV_LAYER: Sign2VecNoLayerNormConvLayer(\n",
      "  (conv): Conv3d(20, 20, kernel_size=(2, 2, 2), stride=(1, 2, 2), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d LAYER: 3\n",
      "LAYER_INPUT: torch.Size([1, 20, 250, 31, 31])\n",
      "CONV_LAYER: Sign2VecNoLayerNormConvLayer(\n",
      "  (conv): Conv3d(20, 20, kernel_size=(2, 2, 2), stride=(1, 2, 2), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n",
      "3d LAYER: 4\n",
      "LAYER_INPUT: torch.Size([1, 20, 249, 15, 15])\n",
      "CONV_LAYER: Sign2VecNoLayerNormConvLayer(\n",
      "  (conv): Conv3d(20, 20, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n",
      "3d LAYER: 5\n",
      "LAYER_INPUT: torch.Size([1, 20, 249, 15, 15])\n",
      "CONV_LAYER: Sign2VecNoLayerNormConvLayer(\n",
      "  (conv): Conv3d(20, 30, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n",
      "3d LAYER: 6\n",
      "LAYER_INPUT: torch.Size([1, 30, 249, 15, 15])\n",
      "CONV_LAYER: Sign2VecNoLayerNormConvLayer(\n",
      "  (conv): Conv3d(30, 5, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n",
      "1d LAYER: 0\n",
      "LAYER_INPUT: torch.Size([1, 1125, 249])\n",
      "CONV_LAYER: Wav2Vec2NoLayerNormConvLayer(\n",
      "  (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "  (activation): GELUActivation()\n",
      ")\n",
      "-------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [512, 512, 3], expected input[1, 1125, 249] to have 512 channels, but got 1125 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m      7\u001b[0m input_video \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\n\u001b[1;32m      8\u001b[0m     batch_size, channels, time, height, width\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m extract_features \u001b[38;5;241m=\u001b[39m feature_extractor(input_video)\n\u001b[1;32m     12\u001b[0m extract_features\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 113\u001b[0m, in \u001b[0;36mSign2VecFeatureEncoder.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    108\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    109\u001b[0m             create_custom_forward(conv_layer),\n\u001b[1;32m    110\u001b[0m             hidden_states,\n\u001b[1;32m    111\u001b[0m         )\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m conv_layer(hidden_states)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:312\u001b[0m, in \u001b[0;36mWav2Vec2NoLayerNormConvLayer.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 312\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(hidden_states)\n\u001b[1;32m    313\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(hidden_states)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    307\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [512, 512, 3], expected input[1, 1125, 249] to have 512 channels, but got 1125 channels instead"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "channels = 3\n",
    "time = 256\n",
    "height = 128\n",
    "width = 128\n",
    "\n",
    "input_video = torch.randn(\n",
    "    batch_size, channels, time, height, width\n",
    ")\n",
    "\n",
    "extract_features = feature_extractor(input_video)\n",
    "extract_features.shape  # (batch_size, num_3d_feat_extract_layers, T, H, W)\n",
    "# extract_features = extract_features.transpose(1, 2)\n",
    "\n",
    "# extract_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 292])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2FeatureEncoder\n",
    "\n",
    "feature_extractor_audio = Wav2Vec2FeatureEncoder(config)\n",
    "\n",
    "with torch.no_grad():\n",
    "    extract_features_audio = feature_extractor_audio(input_values)\n",
    "\n",
    "extract_features_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extract_features = feature_extractor(input_video)\n",
    "extract_features.shape\n",
    "# extract_features = extract_features.transpose(1, 2)\n",
    "\n",
    "# extract_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 93680])\n",
      "torch.Size([1, 292])\n",
      "torch.Size([1, 292, 100])\n"
     ]
    }
   ],
   "source": [
    "print(input_values.shape)\n",
    "print(mask_time_indices.shape)\n",
    "print(sampled_negative_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 30.22791290283203\n",
      "---\n",
      "projected_states torch.Size([1, 292, 256])\n",
      "---\n",
      "projected_quantized_states torch.Size([1, 292, 256])\n",
      "---\n",
      "codevector_perplexity 100.37718200683594\n",
      "---\n",
      "hidden_states no shape\n",
      "---\n",
      "attentions no shape\n",
      "---\n",
      "contrastive_loss 25.59052848815918\n",
      "---\n",
      "diversity_loss 46.37383270263672\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for val in output.__dict__.keys():\n",
    "    try:\n",
    "        print(val, output[val].size() if len(output[val].size()) > 0 else output[val].item())\n",
    "    except:\n",
    "        print(val, 'no shape')\n",
    "    print('---')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
